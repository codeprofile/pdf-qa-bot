{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Retrieval-Augmented Generation (RAG) Model for QA Bot**"
      ],
      "metadata": {
        "id": "JM5_nLqk1im4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Problem Statement:**\n",
        "Develop a Retrieval-Augmented Generation (RAG) model for a Question Answering (QA)\n",
        "bot for a business. Use a vector database like Pinecone DB and a generative model like\n",
        "Cohere API (or any other available alternative). The QA bot should be able to retrieve\n",
        "relevant information from a dataset and generate coherent answers.\n",
        "\n",
        "**Task Requirements:**\n",
        "1. Implement a RAG-based model that can handle questions related to a provided\n",
        "document or dataset.\n",
        "2. Use a vector database (such as Pinecone) to store and retrieve document\n",
        "embeddings efficiently.\n",
        "3. Test the model with several queries and show how well it retrieves and generates\n",
        "accurate answers from the document.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "iLKPa8Nt2qbZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install package requirements"
      ],
      "metadata": {
        "id": "_pHnflmc2B8e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "BzsyanJGdGca",
        "outputId": "2c408f95-8279-42a5-9709-51cd4bcfcd62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pinecone-client in /usr/local/lib/python3.10/dist-packages (5.0.1)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.1.1)\n",
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.10/dist-packages (1.24.10)\n",
            "Requirement already satisfied: cohere in /usr/local/lib/python3.10/dist-packages (5.9.4)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2024.8.30)\n",
            "Requirement already satisfied: pinecone-plugin-inference<2.0.0,>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (1.1.0)\n",
            "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (0.0.7)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.12.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.0.7)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.44.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.4.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.24.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (10.4.0)\n",
            "Requirement already satisfied: PyMuPDFb==1.24.10 in /usr/local/lib/python3.10/dist-packages (from PyMuPDF) (1.24.10)\n",
            "Requirement already satisfied: boto3<2.0.0,>=1.34.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (1.35.24)\n",
            "Requirement already satisfied: fastavro<2.0.0,>=1.9.4 in /usr/local/lib/python3.10/dist-packages (from cohere) (1.9.7)\n",
            "Requirement already satisfied: httpx>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from cohere) (0.27.2)\n",
            "Requirement already satisfied: httpx-sse==0.4.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (0.4.0)\n",
            "Requirement already satisfied: parameterized<0.10.0,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (0.9.0)\n",
            "Requirement already satisfied: pydantic>=1.9.2 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.9.2)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.23.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<1,>=0.15 in /usr/local/lib/python3.10/dist-packages (from cohere) (0.19.1)\n",
            "Requirement already satisfied: types-requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.32.0.20240914)\n",
            "Requirement already satisfied: botocore<1.36.0,>=1.35.24 in /usr/local/lib/python3.10/dist-packages (from boto3<2.0.0,>=1.34.0->cohere) (1.35.24)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3<2.0.0,>=1.34.0->cohere) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from boto3<2.0.0,>=1.34.0->cohere) (0.10.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.21.2->cohere) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.2->cohere) (0.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->cohere) (3.3.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.36.0,>=1.35.24->boto3<2.0.0,>=1.34.0->cohere) (2.8.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.21.2->cohere) (1.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.36.0,>=1.35.24->boto3<2.0.0,>=1.34.0->cohere) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "# Setup\n",
        "!pip install pinecone-client sentence-transformers PyMuPDF cohere"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Import required Libraries"
      ],
      "metadata": {
        "id": "PaSF0yuY2ZfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import os\n",
        "import fitz  # PyMuPDF for PDF processing\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import cohere\n",
        "from typing import List, Tuple"
      ],
      "metadata": {
        "id": "YnwszdCW2fXn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize Pinecone and Cohere"
      ],
      "metadata": {
        "id": "V0mTEW6M2i4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Pinecone and Cohere\n",
        "PINECONE_API_KEY=\"##############################\"  # Replace with your actual Pinecone API key\n",
        "cohere_api_key = '##############################'  # Replace with your actual Cohere API key\n",
        "\n",
        "# Initialize Pinecone\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "INDEX_NAME = 'qa-bot-index'\n",
        "\n",
        "# Delete existing index if it exists\n",
        "if INDEX_NAME in pc.list_indexes().names():\n",
        "    pc.delete_index(INDEX_NAME)\n",
        "\n",
        "# Create a new index\n",
        "pc.create_index(\n",
        "    name=INDEX_NAME,\n",
        "    dimension=384,\n",
        "    metric='euclidean',\n",
        "    spec=ServerlessSpec(cloud='aws', region='us-east-1')\n",
        ")\n",
        "\n",
        "# Connect to the index\n",
        "index = pc.Index(INDEX_NAME)\n",
        "\n",
        "# Load the embedding model\n",
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Initialize Cohere client\n",
        "co = cohere.Client(cohere_api_key)\n"
      ],
      "metadata": {
        "id": "fb3zh6rF2otu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Functions for PDF Processing and Querying"
      ],
      "metadata": {
        "id": "B63L_dKa3ZQA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Functions for PDF Processing and Querying\n",
        "def load_pdf(file_path: str) -> str:\n",
        "    \"\"\"Load text content from a PDF file.\"\"\"\n",
        "    document = fitz.open(file_path)\n",
        "    text = \"\"\n",
        "    for page in document:\n",
        "        text += page.get_text()\n",
        "    return text\n",
        "\n",
        "def preprocess_text(text: str) -> List[str]:\n",
        "    \"\"\"Split the extracted text into manageable segments.\"\"\"\n",
        "    return text.split('\\n')  # Adjust splitting logic as necessary\n",
        "\n",
        "def store_embeddings(documents: List[str]) -> None:\n",
        "    \"\"\"Generate and store embeddings for document segments in Pinecone.\"\"\"\n",
        "    embeddings = embedding_model.encode(documents, convert_to_tensor=True)\n",
        "    for i, (doc, emb) in enumerate(zip(documents, embeddings)):\n",
        "        index.upsert([(str(i), emb.cpu().numpy().tolist())])  # Store each embedding\n",
        "\n",
        "def answer_query(query: str, documents: List[str]) -> Tuple[str, float]:\n",
        "    \"\"\"Retrieve the most relevant document segment based on the user's query.\"\"\"\n",
        "    query_embedding = embedding_model.encode(query, convert_to_tensor=True)\n",
        "    similarities = np.array([\n",
        "        np.dot(query_embedding, emb) / (np.linalg.norm(query_embedding) * np.linalg.norm(emb))\n",
        "        for emb in embedding_model.encode(documents, convert_to_tensor=True).numpy()\n",
        "    ])\n",
        "\n",
        "    best_idx = np.argmax(similarities)\n",
        "    return documents[best_idx], similarities[best_idx]\n",
        "\n",
        "def answer_query_with_cohere(query: str, documents: List[str]) -> str:\n",
        "    \"\"\"Retrieve the most relevant document segment and generate a response using Cohere.\"\"\"\n",
        "    # response_segment, _ = answer_query(query, documents)\n",
        "    prompt = f\"Given the following context from the document:\\n{documents}\\n\\nPlease provide a detailed answer to the question: {query}\\nA:\"\n",
        "\n",
        "    # Generate a response using Cohere\n",
        "    response = co.generate(\n",
        "        model='command-r-plus',  # Choose the appropriate model\n",
        "        prompt=prompt,\n",
        "        max_tokens=150\n",
        "    )\n",
        "\n",
        "    return response.generations[0].text.strip()"
      ],
      "metadata": {
        "id": "n0tVYuBq3SpL"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load PDF and Process"
      ],
      "metadata": {
        "id": "3uFhNQAI3iQf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load PDF and Process\n",
        "pdf_file_path = '/content/Gen AI Engineer _ Machine Learning Engineer Assignment.pdf'  # Replace with your actual PDF path\n",
        "pdf_text = load_pdf(pdf_file_path)  # Load text from PDF\n",
        "documents = preprocess_text(pdf_text)  # Preprocess text into segments\n",
        "store_embeddings(documents)  # Store embeddings in Pinecone"
      ],
      "metadata": {
        "id": "Qnqs_h833jvA"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "dGBCLhl89n49",
        "outputId": "9ed455cf-d05c-4d0d-c02b-683c324e9a8b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Gen AI Engineer / Machine Learning Engineer Assignment\\nPart 1: Retrieval-Augmented Generation (RAG) Model for QA Bot\\nProblem Statement:\\nDevelop a Retrieval-Augmented Generation (RAG) model for a Question Answering (QA)\\nbot for a business. Use a vector database like Pinecone DB and a generative model like\\nCohere API (or any other available alternative). The QA bot should be able to retrieve\\nrelevant information from a dataset and generate coherent answers.\\nTask Requirements:\\n1.\\nImplement a RAG-based model that can handle questions related to a provided\\ndocument or dataset.\\n2.\\nUse a vector database (such as Pinecone) to store and retrieve document\\nembeddings efficiently.\\n3.\\nTest the model with several queries and show how well it retrieves and generates\\naccurate answers from the document.\\nDeliverables:\\n●\\nA Colab notebook demonstrating the entire pipeline, from data loading to question\\nanswering.\\n●\\nDocumentation explaining the model architecture, approach to retrieval, and how\\ngenerative responses are created.\\n●\\nProvide several example queries and the corresponding outputs.\\nPart 2: Interactive QA Bot Interface\\nProblem Statement:\\nDevelop an interactive interface for the QA bot from Part 1, allowing users to input queries\\nand retrieve answers in real time. The interface should enable users to upload documents\\nand ask questions based on the content of the uploaded document.\\nTask Requirements:\\n1.\\nBuild a simple frontend interface using Streamlit or Gradio, allowing users to\\nupload PDF documents and ask questions.\\n2.\\nIntegrate the backend from Part 1 to process the PDF, store document embeddings,\\nand provide real-time answers to user queries.\\n3.\\nEnsure that the system can handle multiple queries efficiently and provide accurate,\\ncontextually relevant responses.\\n4.\\nAllow users to see the retrieved document segments alongside the generated\\nanswer.\\nDeliverables:\\n●\\nA deployed QA bot with a frontend interface where users can upload documents and\\ninteract with the bot.\\n●\\nDocumentation on how the user can upload files, ask questions, and view the bot's\\nresponses.\\n●\\nExample interactions demonstrating the bot's capabilities.\\nGuidelines:\\n●\\nUse Docker to containerize the application for easy deployment.\\n●\\nEnsure the system can handle large documents and multiple queries without\\nsignificant performance drops.\\n●\\nShare the code, deployment instructions, and the final working model through\\nGitHub.\\nGeneral Guidelines:\\n1.\\nEnsure modular and scalable code following best practices for both frontend and\\nbackend development.\\n2.\\nDocument your approach thoroughly, explaining your decisions, challenges faced,\\nand solutions.\\n3.\\nProvide a detailed ReadMe file in your GitHub repository, including setup and usage\\ninstructions.\\n4.\\nSubmissions should include:\\n○\\nSource code for both the notebook and the interface.\\n○\\nA fully functional Colab notebook.\\n○\\nDocumentation on the pipeline and deployment instructions.\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvwjMBCh9sYM",
        "outputId": "a14e30c1-1227-4620-9bee-fe682d7a4478"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Gen AI Engineer / Machine Learning Engineer Assignment',\n",
              " 'Part 1: Retrieval-Augmented Generation (RAG) Model for QA Bot',\n",
              " 'Problem Statement:',\n",
              " 'Develop a Retrieval-Augmented Generation (RAG) model for a Question Answering (QA)',\n",
              " 'bot for a business. Use a vector database like Pinecone DB and a generative model like',\n",
              " 'Cohere API (or any other available alternative). The QA bot should be able to retrieve',\n",
              " 'relevant information from a dataset and generate coherent answers.',\n",
              " 'Task Requirements:',\n",
              " '1.',\n",
              " 'Implement a RAG-based model that can handle questions related to a provided',\n",
              " 'document or dataset.',\n",
              " '2.',\n",
              " 'Use a vector database (such as Pinecone) to store and retrieve document',\n",
              " 'embeddings efficiently.',\n",
              " '3.',\n",
              " 'Test the model with several queries and show how well it retrieves and generates',\n",
              " 'accurate answers from the document.',\n",
              " 'Deliverables:',\n",
              " '●',\n",
              " 'A Colab notebook demonstrating the entire pipeline, from data loading to question',\n",
              " 'answering.',\n",
              " '●',\n",
              " 'Documentation explaining the model architecture, approach to retrieval, and how',\n",
              " 'generative responses are created.',\n",
              " '●',\n",
              " 'Provide several example queries and the corresponding outputs.',\n",
              " 'Part 2: Interactive QA Bot Interface',\n",
              " 'Problem Statement:',\n",
              " 'Develop an interactive interface for the QA bot from Part 1, allowing users to input queries',\n",
              " 'and retrieve answers in real time. The interface should enable users to upload documents',\n",
              " 'and ask questions based on the content of the uploaded document.',\n",
              " 'Task Requirements:',\n",
              " '1.',\n",
              " 'Build a simple frontend interface using Streamlit or Gradio, allowing users to',\n",
              " 'upload PDF documents and ask questions.',\n",
              " '2.',\n",
              " 'Integrate the backend from Part 1 to process the PDF, store document embeddings,',\n",
              " 'and provide real-time answers to user queries.',\n",
              " '3.',\n",
              " 'Ensure that the system can handle multiple queries efficiently and provide accurate,',\n",
              " 'contextually relevant responses.',\n",
              " '4.',\n",
              " 'Allow users to see the retrieved document segments alongside the generated',\n",
              " 'answer.',\n",
              " 'Deliverables:',\n",
              " '●',\n",
              " 'A deployed QA bot with a frontend interface where users can upload documents and',\n",
              " 'interact with the bot.',\n",
              " '●',\n",
              " \"Documentation on how the user can upload files, ask questions, and view the bot's\",\n",
              " 'responses.',\n",
              " '●',\n",
              " \"Example interactions demonstrating the bot's capabilities.\",\n",
              " 'Guidelines:',\n",
              " '●',\n",
              " 'Use Docker to containerize the application for easy deployment.',\n",
              " '●',\n",
              " 'Ensure the system can handle large documents and multiple queries without',\n",
              " 'significant performance drops.',\n",
              " '●',\n",
              " 'Share the code, deployment instructions, and the final working model through',\n",
              " 'GitHub.',\n",
              " 'General Guidelines:',\n",
              " '1.',\n",
              " 'Ensure modular and scalable code following best practices for both frontend and',\n",
              " 'backend development.',\n",
              " '2.',\n",
              " 'Document your approach thoroughly, explaining your decisions, challenges faced,',\n",
              " 'and solutions.',\n",
              " '3.',\n",
              " 'Provide a detailed ReadMe file in your GitHub repository, including setup and usage',\n",
              " 'instructions.',\n",
              " '4.',\n",
              " 'Submissions should include:',\n",
              " '○',\n",
              " 'Source code for both the notebook and the interface.',\n",
              " '○',\n",
              " 'A fully functional Colab notebook.',\n",
              " '○',\n",
              " 'Documentation on the pipeline and deployment instructions.',\n",
              " '']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example Queries"
      ],
      "metadata": {
        "id": "T1UybP3p3ta8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Queries with Cohere\n",
        "test_queries = [\n",
        "    \"what's the documents requirement\",\n",
        "    \"What are the key points discussed in the document?\"\n",
        "]\n",
        "\n",
        "for query in test_queries:\n",
        "    response = answer_query_with_cohere(query, documents)\n",
        "    print(f\"Query: {query}\\nResponse: {response}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oiQhMCnS3urQ",
        "outputId": "d9abdc05-ff25-434d-e9db-bd3fb7802a11"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: what's the documents requirement\n",
            "Response: The document requirements for the assignment are to provide a Colab notebook demonstrating the entire pipeline, from data loading to question answering, and to include documentation explaining the model architecture, approach to retrieval, and how generative responses are created. Additionally, several example queries and their corresponding outputs should be provided.\n",
            "\n",
            "Query: What are the key points discussed in the document?\n",
            "Response: The document outlines a two-part assignment for a Gen AI Engineer or Machine Learning Engineer. \n",
            "\n",
            "Part 1 involves developing a Retrieval-Augmented Generation (RAG) model for a Question Answering (QA) bot. The bot should be able to retrieve relevant information from a dataset and generate coherent answers. The specific tasks include implementing the RAG model, using a vector database for efficient document embedding storage and retrieval, and testing the model with various queries to assess its accuracy. The deliverables for this part include a Colab notebook demonstrating the pipeline, documentation explaining the model architecture and approach, and example queries with corresponding outputs.\n",
            "\n",
            "Part 2 focuses on creating an interactive interface for the QA bot developed in Part 1. The interface\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**README Section**"
      ],
      "metadata": {
        "id": "x17TA7zb3xVv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PDF Question Answering Bot\n",
        "\n",
        "## Overview\n",
        "This notebook implements a Retrieval-Augmented Generation (RAG) model for a Question Answering (QA) bot using Cohere for generative responses and Pinecone for vector storage. The bot processes a PDF document, retrieves relevant information, and allows users to query its content effectively.\n",
        "\n",
        "## Objectives\n",
        "- Develop a QA bot that answers questions based on a provided PDF document.\n",
        "- Utilize Pinecone as a vector database to store and retrieve document embeddings.\n",
        "- Implement Cohere as the generative model to formulate accurate answers.\n",
        "\n",
        "---\n",
        "\n",
        "## Instructions\n",
        "1. **Setup**: Run the first cell to install the required packages:\n",
        "    ```python\n",
        "    !pip install pinecone-client sentence-transformers PyMuPDF cohere\n",
        "    ```\n",
        "   \n",
        "2. **Initialize Pinecone**: Set your Pinecone API key in the designated cell and run it to initialize the Pinecone client and create an index.\n",
        "\n",
        "3. **Set Up Cohere**: Similarly, input your Cohere API key to enable the generative model for answering queries.\n",
        "\n",
        "4. **Define Functions**: The following cells contain functions for:\n",
        "   - Loading and processing PDF documents.\n",
        "   - Storing document embeddings in Pinecone.\n",
        "   - Answering user queries based on the retrieved context.\n",
        "\n",
        "5. **Load PDF**: Provide the path to your PDF file in the designated cell and run it to process the document and store embeddings.\n",
        "\n",
        "6. **Ask Questions**: Use the example queries provided in the last cell or modify them to test the QA bot with your own questions.\n",
        "\n",
        "---\n",
        "\n",
        "## Example Queries\n",
        "- \"What are the key points discussed in the document?\"\n",
        "- \"Can you explain the main arguments presented?\"\n",
        "- \"What conclusions can be drawn from the findings?\"\n",
        "\n",
        "---\n",
        "\n",
        "## Requirements\n",
        "- Python 3.x\n",
        "- Pinecone API key\n",
        "- Cohere API key\n",
        "- PDF document for testing\n",
        "\n",
        "---\n",
        "\n",
        "## Important Notes\n",
        "- Ensure your PDF is structured in a way that allows effective segmentation for accurate answers.\n",
        "- If you encounter issues with responses, consider refining prompts or verifying document content.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "CdhspKc431Jp"
      }
    }
  ]
}